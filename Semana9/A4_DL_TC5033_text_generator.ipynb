{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "037e89c8",
      "metadata": {
        "id": "037e89c8"
      },
      "source": [
        "## TC 5033\n",
        "### Text Generation\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Activity 4: Building a Simple LSTM Text Generator using WikiText-2\n",
        "<br>\n",
        "\n",
        "- Objective:\n",
        "    - Gain a fundamental understanding of Long Short-Term Memory (LSTM) networks.\n",
        "    - Develop hands-on experience with sequence data processing and text generation in PyTorch. Given the simplicity of the model, amount of data, and computer resources, the text you generate will not replace ChatGPT, and results must likely will not make a lot of sense. Its only purpose is academic and to understand the text generation using RNNs.\n",
        "    - Enhance code comprehension and documentation skills by commenting on provided starter code.\n",
        "    \n",
        "<br>\n",
        "\n",
        "- Instructions:\n",
        "    - Code Understanding: Begin by thoroughly reading and understanding the code. Comment each section/block of the provided code to demonstrate your understanding. For this, you are encouraged to add cells with experiments to improve your understanding\n",
        "\n",
        "    - Model Overview: The starter code includes an LSTM model setup for sequence data processing. Familiarize yourself with the model architecture and its components. Once you are familiar with the provided model, feel free to change the model to experiment.\n",
        "\n",
        "    - Training Function: Implement a function to train the LSTM model on the WikiText-2 dataset. This function should feed the training data into the model and perform backpropagation.\n",
        "\n",
        "    - Text Generation Function: Create a function that accepts starting text (seed text) and a specified total number of words to generate. The function should use the trained model to generate a continuation of the input text.\n",
        "\n",
        "    - Code Commenting: Ensure that all the provided starter code is well-commented. Explain the purpose and functionality of each section, indicating your understanding.\n",
        "\n",
        "    - Submission: Submit your Jupyter Notebook with all sections completed and commented. Include a markdown cell with the full names of all contributing team members at the beginning of the notebook.\n",
        "    \n",
        "<br>\n",
        "\n",
        "- Evaluation Criteria:\n",
        "    - Code Commenting (60%): The clarity, accuracy, and thoroughness of comments explaining the provided code. You are suggested to use markdown cells for your explanations.\n",
        "\n",
        "    - Training Function Implementation (20%): The correct implementation of the training function, which should effectively train the model.\n",
        "\n",
        "    - Text Generation Functionality (10%): A working function is provided in comments. You are free to use it as long as you make sure to uderstand it, you may as well improve it as you see fit. The minimum expected is to provide comments for the given function.\n",
        "\n",
        "    - Conclusions (10%): Provide some final remarks specifying the differences you notice between this model and the one used  for classification tasks. Also comment on changes you made to the model, hyperparameters, and any other information you consider relevant. Also, please provide 3 examples of generated texts.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3eb4b117",
      "metadata": {
        "id": "3eb4b117"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "#PyTorch libraries\n",
        "import torch\n",
        "import torchtext\n",
        "from torchtext.datasets import WikiText2\n",
        "# Dataloader library\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.utils.data.dataset import random_split\n",
        "# Libraries to prepare the data\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "# neural layers\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This cell sets an environment variable for PyTorch, specifically configuring how PyTorch should allocate memory on the CUDA (GPU) device. It's setting the maximum split size for CUDA memory allocation to 4096 megabytes, which can help in optimizing memory usage when using GPUs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pYUd0nyKF20C",
      "metadata": {
        "id": "pYUd0nyKF20C"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:4096\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This cell determines whether a CUDA-capable GPU is available for PyTorch. If a GPU is available, it sets the device variable to 'cuda' (which means PyTorch will use the GPU for tensor computations). If not, it falls back to using the CPU ('cpu'). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d8ff971",
      "metadata": {
        "id": "6d8ff971",
        "outputId": "ac9cba6d-5734-432a-d40d-d236c67dac95"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This cell loads the WikiText2 dataset, a widely-used text corpus for language modeling and other natural language processing tasks. The dataset is divided into three parts: training (train_dataset), validation (val_dataset), and testing (test_dataset). These subsets are used respectively for training the model, tuning its hyperparameters, and evaluating its performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3288ce5",
      "metadata": {
        "id": "f3288ce5"
      },
      "outputs": [],
      "source": [
        "train_dataset, val_dataset, test_dataset = WikiText2()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this cell, a tokenizer is set up using PyTorch's get_tokenizer function, specifying 'basic_english' to tokenize the text into words (tokens) based on basic English language rules. The yield_tokens function is defined to iterate over the dataset and yield tokens for each text entry. This function will be used later to build a vocabulary from the dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc4c7dbd",
      "metadata": {
        "id": "fc4c7dbd"
      },
      "outputs": [],
      "source": [
        "tokeniser = get_tokenizer('basic_english')\n",
        "def yield_tokens(data):\n",
        "    for text in data:\n",
        "        yield tokeniser(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c2cb068",
      "metadata": {
        "id": "2c2cb068"
      },
      "outputs": [],
      "source": [
        "# Build the vocabulary\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_dataset), specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"])\n",
        "#set unknown token at position 0\n",
        "vocab.set_default_index(vocab[\"<unk>\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This cell defines a function data_process that converts raw text into sequences of a fixed length (seq_length, set to 50). The function tokenizes the text, converts tokens to their corresponding indices in the vocabulary, and organizes the data into sequences. The sequences are designed so that each input sequence (x_train, x_val, x_test) has a corresponding target sequence (y_train, y_val, y_test) which is the same as the input but offset by one token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "134b832b",
      "metadata": {
        "id": "134b832b"
      },
      "outputs": [],
      "source": [
        "seq_length = 50\n",
        "def data_process(raw_text_iter, seq_length = 50):\n",
        "    data = [torch.tensor(vocab(tokeniser(item)), dtype=torch.long) for item in raw_text_iter]\n",
        "    data = torch.cat(tuple(filter(lambda t: t.numel() > 0, data))) #remove empty tensors\n",
        "#     target_data = torch.cat(d)\n",
        "    return (data[:-(data.size(0)%seq_length)].view(-1, seq_length),\n",
        "            data[1:-(data.size(0)%seq_length-1)].view(-1, seq_length))\n",
        "\n",
        "# # Create tensors for the training set\n",
        "x_train, y_train = data_process(train_dataset, seq_length)\n",
        "x_val, y_val = data_process(val_dataset, seq_length)\n",
        "x_test, y_test = data_process(test_dataset, seq_length)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b54c04d",
      "metadata": {
        "id": "4b54c04d"
      },
      "outputs": [],
      "source": [
        "train_dataset = TensorDataset(x_train, y_train)\n",
        "val_dataset = TensorDataset(x_val, y_val)\n",
        "test_dataset = TensorDataset(x_test, y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This cell creates DataLoader objects for the training, validation, and test datasets. The DataLoader is responsible for batching the data (in this case, batch size is set to 32) and shuffling it for training. The drop_last=True parameter ensures that any incomplete batch (smaller than the batch size) at the end of the dataset is dropped."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4d400fb",
      "metadata": {
        "id": "f4d400fb"
      },
      "outputs": [],
      "source": [
        "batch_size = 32  # choose a batch size that fits your computation resources\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- This cell defines a class LSTMModel using PyTorch's neural network module (nn.Module). The model consists of an embedding layer (nn.Embedding), an LSTM layer (nn.LSTM), a dropout layer (nn.Dropout for regularization), and a fully connected layer (nn.Linear).\n",
        "- The forward method defines the forward pass for the network: input text is first passed through the embedding layer, then the LSTM layer, followed by the dropout and fully-connected layers. The LSTM's hidden state is also managed in this method.\n",
        "- The init_hidden method initializes the hidden states for the LSTM layer, which is necessary for the first forward pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59c63b01",
      "metadata": {
        "id": "59c63b01",
        "outputId": "a21934e6-36c8-4acd-9c32-f2a88d8d6057"
      },
      "outputs": [],
      "source": [
        "class LSTMModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_size, n_hidden=256, n_layers=4, drop_prob=0.3, lr=0.001):\n",
        "        super().__init__()\n",
        "\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "\n",
        "        self.emb_layer = nn.Embedding(vocab_size, embed_size)\n",
        "\n",
        "        ## define the LSTM\n",
        "        self.lstm = nn.LSTM(embed_size, n_hidden, n_layers,\n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "\n",
        "        ## define a dropout layer\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "\n",
        "        ## define the fully-connected layer\n",
        "        self.fc = nn.Linear(n_hidden, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        ''' Forward pass through the network.\n",
        "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
        "\n",
        "        ## pass input through embedding layer\n",
        "        embedded = self.emb_layer(x)\n",
        "\n",
        "        ## Get the outputs and the new hidden state from the lstm\n",
        "        lstm_output, hidden = self.lstm(embedded, hidden)\n",
        "\n",
        "        ## pass through a dropout layer , this layer is used to regularize, avoiding overfitting in the model\n",
        "        out = self.dropout(lstm_output)\n",
        "\n",
        "        #out = out.contiguous().view(-1, self.n_hidden)\n",
        "        out = out.reshape(-1, self.n_hidden)\n",
        "\n",
        "        ## put \"out\" through the fully-connected layer\n",
        "        out = self.fc(out)\n",
        "\n",
        "        # return the final output and the hidden state\n",
        "        return out, hidden\n",
        "\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "\n",
        "        # if GPU is available\n",
        "        if (torch.cuda.is_available()):\n",
        "          hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                    weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "\n",
        "        # if GPU is not available\n",
        "        else:\n",
        "          hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                    weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "\n",
        "        return hidden\n",
        "\n",
        "vocab_size = len(vocab) # vocabulary size\n",
        "emb_size = 100 # embedding size\n",
        "neurons = 128 # the dimension of the feedforward network model, i.e. # of neurons\n",
        "num_layers = 1 # the number of nn.LSTM layers\n",
        "drop_prob=0.3\n",
        "lr = 0.001\n",
        "model = LSTMModel(vocab_size, emb_size, neurons, num_layers, drop_prob, lr)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Trainning Loop\n",
        "\n",
        "- This cell provides a template for a training function. It includes instructions and placeholders for the key steps in training a neural network, such as looping through epochs, handling data loading, initializing hidden states, running the model forward pass, computing loss, performing backpropagation, and updating parameters.\n",
        "\n",
        "\n",
        "- The cell outlines the use of a loss function (nn.CrossEntropyLoss), transferring data to the correct device (GPU or CPU), detaching hidden states, and zeroing gradients. It also includes a step for gradient clipping, which is a common technique to prevent exploding gradients in RNNs and LSTMs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "215eabb9",
      "metadata": {
        "id": "215eabb9"
      },
      "outputs": [],
      "source": [
        "def train(model, epochs, optimiser, clip=1):\n",
        "    '''\n",
        "    The following are possible instructions you may want to conside for this function.\n",
        "    This is only a guide and you may change add or remove whatever you consider appropriate\n",
        "    as long as you train your model correctly.\n",
        "        - loop through specified epochs\n",
        "        - loop through dataloader\n",
        "        - don't forget to zero grad!\n",
        "        - place data (both input and target) in device\n",
        "        - init hidden states e.g. hidden = model.init_hidden(batch_size)\n",
        "        - run the model\n",
        "        - compute the cost or loss\n",
        "        - backpropagation\n",
        "        - Update paratemers\n",
        "        - Include print all the information you consider helpful\n",
        "\n",
        "    '''\n",
        "\n",
        "    # loss\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    model = model.to(device=device)\n",
        "    model.train()\n",
        "\n",
        "    for e in range(epochs):\n",
        "\n",
        "        # initialize hidden state\n",
        "        h = model.init_hidden(batch_size)\n",
        "        losses = []\n",
        "        for x, y in tqdm(train_loader):\n",
        "\n",
        "            # push tensors to GPU\n",
        "            inputs, targets = x.cuda(), y.cuda()\n",
        "\n",
        "            # detach hidden states\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            # zero accumulated gradients\n",
        "            model.zero_grad()\n",
        "\n",
        "            # get the output from the model\n",
        "            output, h = model.forward(inputs, h)\n",
        "\n",
        "            # calculate the loss and perform backprop\n",
        "            loss = criterion(output, targets.view(-1))\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            # back-propagate error\n",
        "            loss.backward()\n",
        "\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "            # update weigths\n",
        "            optimiser.step()\n",
        "\n",
        "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
        "        #CalcValLossAndAccuracy(model, loss_fn, val_loader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initialization\n",
        "\n",
        "This cell prepares for the training of the LSTM model. It sets up the loss function (nn.CrossEntropyLoss), defines the number of training epochs, and initializes the optimizer (optim.Adam), specifying the learning rate and model parameters.\n",
        "It then calls the training function with the model, number of epochs, and optimizer, effectively starting the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa9c84ce",
      "metadata": {
        "id": "aa9c84ce",
        "outputId": "c9fa57a2-af86-4cb4-f838-2b95ae8ffacb"
      },
      "outputs": [],
      "source": [
        "# Call the train function\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "#lr = 0.0005\n",
        "epochs = 5\n",
        "optimiser = optim.Adam(model.parameters(), lr=lr)\n",
        "train(model, epochs, optimiser)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Predictions\n",
        "This cell defines two functions for text generation using the trained model. The first function, predict, takes the model (net), a token (tkn), and optionally a hidden state (h). It predicts the next token based on the input.\n",
        "\n",
        "- The second function, sample, generates a sequence of text of a specified size, starting with a given initial text (prime). It uses the predict function to generate each subsequent token.\n",
        "\n",
        "- The sample function is demonstrated at the end of the cell with an example, generating text starting with \"I like\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zbvvs8NmF20F",
      "metadata": {
        "id": "zbvvs8NmF20F",
        "outputId": "fff6baf0-0d8f-43cb-a6c0-5d2e51b55fed"
      },
      "outputs": [],
      "source": [
        "# predict next token\n",
        "def predict(net, tkn, h=None):\n",
        "\n",
        "  # tensor inputs\n",
        "  x = np.array([vocab([tkn])])\n",
        "  inputs = torch.from_numpy(x)\n",
        "\n",
        "  # push to GPU\n",
        "  inputs = inputs.cuda()\n",
        "\n",
        "  # detach hidden state from history\n",
        "  h = tuple([each.data for each in h])\n",
        "\n",
        "  # get the output of the model\n",
        "  out, h = net(inputs, h)\n",
        "\n",
        "  # get the token probabilities\n",
        "  p = F.softmax(out, dim=1).data\n",
        "\n",
        "  p = p.cpu()\n",
        "\n",
        "  p = p.numpy()\n",
        "  p = p.reshape(p.shape[1],)\n",
        "\n",
        "  # get indices of top 3 values\n",
        "  top_n_idx = p.argsort()[-3:][::-1]\n",
        "\n",
        "  # randomly select one of the three indices\n",
        "  sampled_token_index = top_n_idx[random.sample([0,1,2],1)[0]]\n",
        "\n",
        "  # return the encoded value of the predicted char and the hidden state\n",
        "\n",
        "  return vocab.get_itos()[sampled_token_index], h\n",
        "\n",
        "\n",
        "# function to generate text\n",
        "def sample(net, size, prime='it is'):\n",
        "\n",
        "    # push to GPU\n",
        "    net.cuda()\n",
        "\n",
        "    net.eval()\n",
        "\n",
        "    # batch size is 1\n",
        "    h = net.init_hidden(1)\n",
        "\n",
        "    toks = prime.split()\n",
        "\n",
        "    # predict next token\n",
        "    for t in prime.split():\n",
        "      token, h = predict(net, t, h)\n",
        "\n",
        "    toks.append(token)\n",
        "\n",
        "    # predict subsequent tokens\n",
        "    for i in range(size-1):\n",
        "        token, h = predict(net, toks[-1], h)\n",
        "        toks.append(token)\n",
        "\n",
        "    return ' '.join(toks)\n",
        "\n",
        "print(sample(model, size=100, prime=\"I like\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3 Examples\n",
        "- I like a . muscaria , and the most important of the song ' s work is not to be a <unk> of <unk> , and <unk> <unk> <unk> <unk> , the first of <unk> . = <unk> <unk> = the first <unk> ( the <unk> of the <unk> <unk> <unk> , the song , and <unk> . in the same period . = <unk> <unk> ( <unk> – the <unk> , the song <unk> ) = = in a number , he also had been a small <unk> , and <unk> <unk> <unk> <unk> . = the <unk> <unk> ( <unk>\n",
        "\n",
        "- I like the song was the first of the <unk> , and the <unk> <unk> <unk> <unk> , the <unk> of <unk> and the other of <unk> . in the <unk> , <unk> <unk> , the first , the song is the most common and a new <unk> , and the most important <unk> <unk> <unk> . the first <unk> , and the <unk> of the song ' <unk> <unk> ( a <unk> <unk> ) and the song was the first to the song , which had the first time in the first game in the united kingdom . = = =\n",
        "\n",
        "- I like a . hygrometricus and <unk> . <unk> and <unk> . the first two of a new <unk> , and <unk> of <unk> , the song is a small <unk> of a <unk> , and a few days of the <unk> , and a new <unk> and a <unk> of a new york city of <unk> and a new <unk> , the first time in a new <unk> of the song , he is also a <unk> of the first time in his first time in the same day . = = reception = in the <unk> , the song is"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusion\n",
        "\n",
        "## Model Purpose and Design\n",
        "- Designed: to predict the next token in a sequence, essentially generating text one token at a time. It's typically used for tasks like language modeling and text generation.\n",
        "\n",
        "In contrast, a context classification model aims to understand or categorize the entire input text into predefined classes. It's often used in applications like sentiment analysis or topic classification. Such models might use various architectures, including CNNs (Convolutional Neural Networks), RNNs (Recurrent Neural Networks), or Transformer-based models for complex tasks. The focus here is on extracting and interpreting the overall meaning of the input text, rather than generating new content. Training of these models involves processing the entire text input at once, with the model learning to associate input patterns with specific output classes. The loss calculation, often using Cross-Entropy Loss, is based on the accuracy of classifying the entire text into the correct category.\n",
        "\n",
        "\n",
        "## The contrast for this approach\n",
        "\n",
        "The choice of hyperparameters also reflects the distinct nature of these models. For the text generation model, parameters like sequence length, the number and size of hidden layers in the LSTM, embedding size, and dropout rate are crucial. These parameters determine how much context the model considers and its ability to remember information over sequences, as well as managing overfitting. For context classification models, parameters such as the size and number of filters in CNNs, the number of hidden units in RNNs, and the presence of pooling layers in CNNs play a significant role. \n",
        "\n",
        "These models might also leverage attention mechanisms, especially in Transformer-based models, to focus on relevant parts of the input for accurate classification. Additionally, context classification models often benefit from transfer learning, using pre-trained models on large datasets to achieve a better understanding of the text context.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cb126a2",
      "metadata": {
        "id": "1cb126a2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6de373d8",
      "metadata": {
        "id": "6de373d8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68d82438",
      "metadata": {
        "id": "68d82438"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
